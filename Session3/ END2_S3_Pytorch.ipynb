{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END2_S3_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8bCOG1mUr9zavZq9rc/UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikiiitb2013/END2_p1/blob/main/Session3/%20END2_S3_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWKb41LmaYp8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from random import randint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_YwbFHTalfp",
        "outputId": "491fe302-860a-4943-9eaf-2fdea9981414"
      },
      "source": [
        "# def get_device():\n",
        "#     if torch.cuda.is_available():\n",
        "#         device = 'cuda:0'\n",
        "#     else:\n",
        "#         device = 'cpu'\n",
        "#     return device\n",
        "# device = get_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XtjWlglXvdN"
      },
      "source": [
        "# sample_data = np.arange(0, 10)\n",
        "# print('The whole data: ', sample_data)\n",
        "# dataset = ExampleDataset(sample_data)\n",
        "# print('Number of samples in the data: ', len(dataset))\n",
        "# print(dataset[2])\n",
        "# print(dataset[0:5])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz_yjYWMYOeW",
        "outputId": "f896493a-4371-4236-eeb2-f7a71681670b"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/mnist_train.csv\n",
        "!wget https://pjreddie.com/media/files/mnist_test.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-21 13:39:31--  https://pjreddie.com/media/files/mnist_train.csv\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109575994 (104M) [application/octet-stream]\n",
            "Saving to: ‘mnist_train.csv’\n",
            "\n",
            "mnist_train.csv     100%[===================>] 104.50M  35.3MB/s    in 3.0s    \n",
            "\n",
            "2021-05-21 13:39:34 (35.3 MB/s) - ‘mnist_train.csv’ saved [109575994/109575994]\n",
            "\n",
            "--2021-05-21 13:39:34--  https://pjreddie.com/media/files/mnist_test.csv\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18289443 (17M) [application/octet-stream]\n",
            "Saving to: ‘mnist_test.csv’\n",
            "\n",
            "mnist_test.csv      100%[===================>]  17.44M  24.3MB/s    in 0.7s    \n",
            "\n",
            "2021-05-21 13:39:35 (24.3 MB/s) - ‘mnist_test.csv’ saved [18289443/18289443]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYvRGb8sbadM",
        "outputId": "861fddca-8f89-40c7-e49c-1ec23b01fa63"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist_test.csv\tmnist_train.csv  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P89S8Iz4bg2B"
      },
      "source": [
        "df_train = pd.read_csv('mnist_train.csv', header=None)\n",
        "df_test = pd.read_csv('mnist_test.csv', header=None)\n",
        "\n",
        "# get the image pixel values and labels\n",
        "# train_labels = df_train.iloc[:, 0]\n",
        "# train_images = df_train.iloc[:, 1:]\n",
        "# test_labels = df_test.iloc[:, 0]\n",
        "# test_images = df_test.iloc[:, 1:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQlSBEDocj8M",
        "outputId": "4ce29aa1-cef5-4993-f386-2900c1a2bd49"
      },
      "source": [
        "# custom dataset\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class customMNISTDataset(Dataset):\n",
        "  def __init__(self, data, transforms=None):\n",
        "    self.X = data.iloc[:, 1:]\n",
        "    self.y = data.iloc[:, 0]\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_data = self.X.iloc[idx, :]\n",
        "    # if isinstance(idx, slice):\n",
        "    #     img_data = np.array([i.reshape(1,28, 28) for i in np.array(img_data)])\n",
        "    #     rand_no = np.array([randint(0, 9) for i in np.array(img_data)])\n",
        "    #     y1 = np.array(self.y[idx])\n",
        "    #     y2 = y1 + rand_no\n",
        "    # else:\n",
        "    img_data = np.array(img_data).astype(np.uint8).reshape(28, 28, 1)\n",
        "    rand_no = randint(0, 9)\n",
        "    y1 = self.y[idx]\n",
        "    y2 = y1 + rand_no\n",
        "    \n",
        "    if self.transforms:\n",
        "      img_data = self.transforms(img_data)\n",
        "    return img_data, rand_no, y1, y2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_ds = customMNISTDataset(df_train, transform)\n",
        "test_ds = customMNISTDataset(df_test, transform)\n",
        "print('Number of samples in the train data: ', len(train_ds))\n",
        "print('Number of samples in the test data: ', len(test_ds))\n",
        "\n",
        "trainloader = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
        "aa = next(iter(trainloader))\n",
        "# aa[1].type()\n",
        "# print(dataset[2])\n",
        "# print(dataset[0:5])\n",
        "\n",
        "# batch = next(iter(trainloader))\n",
        "\n",
        "# len(batch), type(batch)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in the train data:  60000\n",
            "Number of samples in the test data:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1UkeDONhlww"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_block(input_size, output_size, kernel_size):\n",
        "    block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=kernel_size), nn.ReLU(), nn.MaxPool2d((2, 2)),\n",
        "    )\n",
        "    return block\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = conv_block(input_size=1, output_size=6, kernel_size=3)\n",
        "    self.conv2 = conv_block(input_size=6, output_size=12, kernel_size=3)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(in_features=12*5*5, out_features=16)\n",
        "    self.fc2 = nn.Linear(in_features=16, out_features=5)\n",
        "\n",
        "    self.fc3 = nn.Linear(in_features=10, out_features=10)\n",
        "    self.fc4 = nn.Linear(in_features=10, out_features=5)\n",
        "\n",
        "    self.fc5 = nn.Linear(in_features=10, out_features=50)\n",
        "    self.fc6 = nn.Linear(in_features=50, out_features=29)\n",
        "\n",
        "\n",
        "  def forward(self, img, ohe):\n",
        "\n",
        "    img = self.conv1(img)\n",
        "    img = self.conv2(img)\n",
        "    img = img.reshape(img.shape[0], -1)  # img = img.reshape(-1, 12*5*5)\n",
        "    img = self.relu(self.fc1(img))\n",
        "    img = self.relu(self.fc2(img))\n",
        "\n",
        "    ohe = self.relu(self.fc3(ohe))\n",
        "    ohe = self.relu(self.fc4(ohe))\n",
        "    \n",
        "    x = torch.cat((img, ohe), dim=1)\n",
        "    x = self.relu(x)\n",
        "    x = self.relu(self.fc5(x))\n",
        "    x = self.fc6(x)\n",
        "    return x[:,0:10],x[:,10:]\n",
        "    # return F.softmax(x[:,0:10], dim=1),F.softmax(x[:,10:], dim=1)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXn579d4S_ho",
        "outputId": "a56a528f-f649-42aa-fed7-fc62fe04cf23"
      },
      "source": [
        "sample = next(iter(train_ds)) \n",
        "image, randno, label, sm = sample\n",
        "\n",
        "print(image.shape, image.unsqueeze(0).shape, randno, label, sm)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) torch.Size([1, 1, 28, 28]) 7 5 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tevaX7erUOwx",
        "outputId": "b14e0c76-971a-4f5b-db03-c10618b9fb42"
      },
      "source": [
        "network = Network()\n",
        "pred1,pred2 = network(image.unsqueeze(0), F.one_hot(torch.tensor(randno).unsqueeze(0), num_classes=10).float())\n",
        "print(pred1.sum(), pred2.sum())\n",
        "pred1,pred2\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3265, grad_fn=<SumBackward0>) tensor(-0.2265, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0612,  0.2506,  0.0965, -0.1181,  0.1350,  0.0673, -0.0616,  0.0418,\n",
              "           0.0790, -0.1029]], grad_fn=<SliceBackward>),\n",
              " tensor([[ 0.0967, -0.1094, -0.1569, -0.1711,  0.0335, -0.1528, -0.0331,  0.0831,\n",
              "           0.0807,  0.0204, -0.0438,  0.2704,  0.0534, -0.0263, -0.1500,  0.0234,\n",
              "          -0.0539,  0.0748, -0.0660]], grad_fn=<SliceBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv3hNRC5U8rx",
        "outputId": "46b907b0-c215-4433-d4d5-141b1fe427cb"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds, \n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(len(batch))\n",
        "batch\n",
        "\n",
        "images, randnos, labels, sums = batch\n",
        "\n",
        "pred1,pred2 = network(images, F.one_hot(torch.tensor(randnos), num_classes=10).float())\n",
        "print(pred1.sum(dim = 1), pred2.sum(dim = 1))\n",
        "print(pred1.shape,pred2.shape)\n",
        "pred1.argmax(dim=1).eq(labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "tensor([0.3111, 0.3383, 0.2910, 0.3487, 0.3480, 0.3086, 0.3154, 0.3501, 0.3280,\n",
            "        0.3471, 0.3273, 0.3259, 0.3263, 0.3079, 0.3550, 0.3064, 0.3523, 0.3456,\n",
            "        0.3521, 0.2966, 0.3534, 0.3219, 0.3071, 0.2933, 0.3417, 0.3467, 0.3092,\n",
            "        0.3384, 0.2885, 0.3272, 0.2988, 0.2885], grad_fn=<SumBackward1>) tensor([-0.2285, -0.2356, -0.2447, -0.2268, -0.2228, -0.2296, -0.2344, -0.2212,\n",
            "        -0.2335, -0.2229, -0.2293, -0.2276, -0.2310, -0.2334, -0.2266, -0.2310,\n",
            "        -0.2165, -0.2201, -0.2291, -0.2374, -0.2130, -0.2364, -0.2304, -0.2384,\n",
            "        -0.2257, -0.2264, -0.2327, -0.2282, -0.2362, -0.2291, -0.2419, -0.2433],\n",
            "       grad_fn=<SumBackward1>)\n",
            "torch.Size([32, 10]) torch.Size([32, 19])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False, False,  True, False, False,  True, False,  True, False,\n",
              "        False, False, False, False,  True, False, False, False, False, False,\n",
              "        False, False, False,  True,  True, False, False, False, False, False,\n",
              "        False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pumo-LdauLa"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivzlKM9gVMpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f027937-0868-44f4-cea5-3bc261ee425f"
      },
      "source": [
        "model = Network().to(device)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    total_loss = 0\n",
        "    total_loss_t = 0\n",
        "    total_correct_images = 0\n",
        "    total_correct_sums = 0\n",
        "    total_correct_images_t = 0\n",
        "    total_correct_sums_t = 0\n",
        "    \n",
        "    # train the model #\n",
        "    model.train()\n",
        "    for batch in train_loader: # Get Batch\n",
        "        images, randnos, labels, sums = batch\n",
        "        images, randnos, labels, sums = images.to(device), randnos.to(device), labels.to(device), sums.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds1,preds2 = model(images,F.one_hot(torch.tensor(randnos), num_classes=10).float()) # Pass Batch\n",
        "        loss1 = F.cross_entropy(preds1, labels) # Calculate Loss\n",
        "        loss2 = F.cross_entropy(preds2, sums) # Calculate Loss\n",
        "        loss = (19*(loss1) + 10*(loss2))/29\n",
        "        # loss = loss1 + loss2\n",
        "        \n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct_images += get_num_correct(preds1, labels)\n",
        "        total_correct_sums += get_num_correct(preds2, sums)\n",
        "    \n",
        "\n",
        "    print( \"epoch\", epoch, \n",
        "        \"total_correct(images):\", total_correct_images,\"/\",len(train_ds),\n",
        "        \"total_correct(sum):\" , total_correct_sums,\"/\",len(train_ds),\n",
        "        \"loss:\", total_loss\n",
        "    )\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 total_correct(images): 22855 / 60000 total_correct(sum): 5665 / 60000 loss: 7807.050562977791\n",
            "epoch 1 total_correct(images): 41702 / 60000 total_correct(sum): 5935 / 60000 loss: 5764.25578212738\n",
            "epoch 2 total_correct(images): 47466 / 60000 total_correct(sum): 6548 / 60000 loss: 4982.4205040335655\n",
            "epoch 3 total_correct(images): 49880 / 60000 total_correct(sum): 8180 / 60000 loss: 4508.7538805007935\n",
            "epoch 4 total_correct(images): 51578 / 60000 total_correct(sum): 9660 / 60000 loss: 4060.484685897827\n",
            "epoch 5 total_correct(images): 52944 / 60000 total_correct(sum): 11323 / 60000 loss: 3693.761387348175\n",
            "epoch 6 total_correct(images): 54010 / 60000 total_correct(sum): 13394 / 60000 loss: 3389.1274438500404\n",
            "epoch 7 total_correct(images): 54736 / 60000 total_correct(sum): 16223 / 60000 loss: 3133.316538631916\n",
            "epoch 8 total_correct(images): 55339 / 60000 total_correct(sum): 18751 / 60000 loss: 2928.797610759735\n",
            "epoch 9 total_correct(images): 55724 / 60000 total_correct(sum): 21089 / 60000 loss: 2764.721148252487\n",
            "epoch 10 total_correct(images): 56037 / 60000 total_correct(sum): 23396 / 60000 loss: 2627.238964945078\n",
            "epoch 11 total_correct(images): 56282 / 60000 total_correct(sum): 25322 / 60000 loss: 2517.0053839981556\n",
            "epoch 12 total_correct(images): 56448 / 60000 total_correct(sum): 27333 / 60000 loss: 2410.4136097729206\n",
            "epoch 13 total_correct(images): 56596 / 60000 total_correct(sum): 29004 / 60000 loss: 2321.953174650669\n",
            "epoch 14 total_correct(images): 56715 / 60000 total_correct(sum): 30734 / 60000 loss: 2240.907633394003\n",
            "epoch 15 total_correct(images): 56868 / 60000 total_correct(sum): 31989 / 60000 loss: 2158.3192783892155\n",
            "epoch 16 total_correct(images): 56941 / 60000 total_correct(sum): 33398 / 60000 loss: 2085.970966219902\n",
            "epoch 17 total_correct(images): 57032 / 60000 total_correct(sum): 34418 / 60000 loss: 2023.8528337478638\n",
            "epoch 18 total_correct(images): 57097 / 60000 total_correct(sum): 35769 / 60000 loss: 1951.671406775713\n",
            "epoch 19 total_correct(images): 57146 / 60000 total_correct(sum): 37004 / 60000 loss: 1893.907660678029\n",
            "epoch 20 total_correct(images): 57249 / 60000 total_correct(sum): 38267 / 60000 loss: 1837.9666854590178\n",
            "epoch 21 total_correct(images): 57329 / 60000 total_correct(sum): 39222 / 60000 loss: 1780.6781024187803\n",
            "epoch 22 total_correct(images): 57315 / 60000 total_correct(sum): 40136 / 60000 loss: 1733.0326383560896\n",
            "epoch 23 total_correct(images): 57437 / 60000 total_correct(sum): 41174 / 60000 loss: 1686.9023640155792\n",
            "epoch 24 total_correct(images): 57494 / 60000 total_correct(sum): 41940 / 60000 loss: 1634.0426862984896\n",
            "epoch 25 total_correct(images): 57543 / 60000 total_correct(sum): 42559 / 60000 loss: 1590.0974762141705\n",
            "epoch 26 total_correct(images): 57543 / 60000 total_correct(sum): 43384 / 60000 loss: 1549.0360897481441\n",
            "epoch 27 total_correct(images): 57616 / 60000 total_correct(sum): 44046 / 60000 loss: 1511.6813726872206\n",
            "epoch 28 total_correct(images): 57646 / 60000 total_correct(sum): 44615 / 60000 loss: 1478.2972678989172\n",
            "epoch 29 total_correct(images): 57712 / 60000 total_correct(sum): 45296 / 60000 loss: 1439.268848143518\n",
            "epoch 30 total_correct(images): 57745 / 60000 total_correct(sum): 45592 / 60000 loss: 1411.0820704251528\n",
            "epoch 31 total_correct(images): 57746 / 60000 total_correct(sum): 46022 / 60000 loss: 1378.8201094269753\n",
            "epoch 32 total_correct(images): 57816 / 60000 total_correct(sum): 46547 / 60000 loss: 1355.6427497267723\n",
            "epoch 33 total_correct(images): 57849 / 60000 total_correct(sum): 47134 / 60000 loss: 1315.2220765277743\n",
            "epoch 34 total_correct(images): 57905 / 60000 total_correct(sum): 47686 / 60000 loss: 1283.5130268707871\n",
            "epoch 35 total_correct(images): 57928 / 60000 total_correct(sum): 48025 / 60000 loss: 1249.1159695610404\n",
            "epoch 36 total_correct(images): 57923 / 60000 total_correct(sum): 48503 / 60000 loss: 1224.6107589676976\n",
            "epoch 37 total_correct(images): 57938 / 60000 total_correct(sum): 48948 / 60000 loss: 1197.4583518728614\n",
            "epoch 38 total_correct(images): 57992 / 60000 total_correct(sum): 49340 / 60000 loss: 1169.957283206284\n",
            "epoch 39 total_correct(images): 58060 / 60000 total_correct(sum): 49440 / 60000 loss: 1147.0308940410614\n",
            "epoch 40 total_correct(images): 58060 / 60000 total_correct(sum): 49829 / 60000 loss: 1132.9956077635288\n",
            "epoch 41 total_correct(images): 58130 / 60000 total_correct(sum): 50154 / 60000 loss: 1101.4419314563274\n",
            "epoch 42 total_correct(images): 58130 / 60000 total_correct(sum): 50517 / 60000 loss: 1086.4023428298533\n",
            "epoch 43 total_correct(images): 58171 / 60000 total_correct(sum): 50804 / 60000 loss: 1062.2552783116698\n",
            "epoch 44 total_correct(images): 58161 / 60000 total_correct(sum): 51038 / 60000 loss: 1040.5664383471012\n",
            "epoch 45 total_correct(images): 58206 / 60000 total_correct(sum): 51240 / 60000 loss: 1020.2230875939131\n",
            "epoch 46 total_correct(images): 58164 / 60000 total_correct(sum): 51450 / 60000 loss: 1003.5838406458497\n",
            "epoch 47 total_correct(images): 58209 / 60000 total_correct(sum): 51741 / 60000 loss: 981.6787337474525\n",
            "epoch 48 total_correct(images): 58230 / 60000 total_correct(sum): 51976 / 60000 loss: 968.2554481215775\n",
            "epoch 49 total_correct(images): 58292 / 60000 total_correct(sum): 52096 / 60000 loss: 948.8871622681618\n",
            "epoch 50 total_correct(images): 58301 / 60000 total_correct(sum): 52357 / 60000 loss: 932.5019534230232\n",
            "epoch 51 total_correct(images): 58251 / 60000 total_correct(sum): 52437 / 60000 loss: 919.6560643650591\n",
            "epoch 52 total_correct(images): 58300 / 60000 total_correct(sum): 52724 / 60000 loss: 899.340486805886\n",
            "epoch 53 total_correct(images): 58351 / 60000 total_correct(sum): 52875 / 60000 loss: 878.7015183418989\n",
            "epoch 54 total_correct(images): 58352 / 60000 total_correct(sum): 53102 / 60000 loss: 866.5813536606729\n",
            "epoch 55 total_correct(images): 58343 / 60000 total_correct(sum): 53302 / 60000 loss: 856.159958217293\n",
            "epoch 56 total_correct(images): 58374 / 60000 total_correct(sum): 53501 / 60000 loss: 830.9526519551873\n",
            "epoch 57 total_correct(images): 58407 / 60000 total_correct(sum): 53743 / 60000 loss: 816.7696008235216\n",
            "epoch 58 total_correct(images): 58425 / 60000 total_correct(sum): 53795 / 60000 loss: 804.2502889111638\n",
            "epoch 59 total_correct(images): 58427 / 60000 total_correct(sum): 54048 / 60000 loss: 792.270838195458\n",
            "epoch 60 total_correct(images): 58405 / 60000 total_correct(sum): 54155 / 60000 loss: 778.7275005150586\n",
            "epoch 61 total_correct(images): 58433 / 60000 total_correct(sum): 54283 / 60000 loss: 765.2339031230658\n",
            "epoch 62 total_correct(images): 58457 / 60000 total_correct(sum): 54549 / 60000 loss: 749.3451024852693\n",
            "epoch 63 total_correct(images): 58486 / 60000 total_correct(sum): 54702 / 60000 loss: 732.7243029531091\n",
            "epoch 64 total_correct(images): 58474 / 60000 total_correct(sum): 54823 / 60000 loss: 721.2585965916514\n",
            "epoch 65 total_correct(images): 58475 / 60000 total_correct(sum): 54994 / 60000 loss: 699.1032366920263\n",
            "epoch 66 total_correct(images): 58489 / 60000 total_correct(sum): 55055 / 60000 loss: 693.2092374414206\n",
            "epoch 67 total_correct(images): 58486 / 60000 total_correct(sum): 55364 / 60000 loss: 673.8872648179531\n",
            "epoch 68 total_correct(images): 58535 / 60000 total_correct(sum): 55592 / 60000 loss: 653.2858147714287\n",
            "epoch 69 total_correct(images): 58531 / 60000 total_correct(sum): 55800 / 60000 loss: 632.7052873149514\n",
            "epoch 70 total_correct(images): 58534 / 60000 total_correct(sum): 56059 / 60000 loss: 613.8695504628122\n",
            "epoch 71 total_correct(images): 58550 / 60000 total_correct(sum): 56178 / 60000 loss: 593.2711629765108\n",
            "epoch 72 total_correct(images): 58567 / 60000 total_correct(sum): 56236 / 60000 loss: 578.146983970888\n",
            "epoch 73 total_correct(images): 58601 / 60000 total_correct(sum): 56488 / 60000 loss: 565.0143378758803\n",
            "epoch 74 total_correct(images): 58604 / 60000 total_correct(sum): 56625 / 60000 loss: 546.0997259346768\n",
            "epoch 75 total_correct(images): 58614 / 60000 total_correct(sum): 56689 / 60000 loss: 530.0479660760611\n",
            "epoch 76 total_correct(images): 58601 / 60000 total_correct(sum): 56926 / 60000 loss: 512.3680435298011\n",
            "epoch 77 total_correct(images): 58623 / 60000 total_correct(sum): 57068 / 60000 loss: 506.598953246139\n",
            "epoch 78 total_correct(images): 58662 / 60000 total_correct(sum): 57020 / 60000 loss: 494.1625403170474\n",
            "epoch 79 total_correct(images): 58660 / 60000 total_correct(sum): 57265 / 60000 loss: 477.1597993527539\n",
            "epoch 80 total_correct(images): 58674 / 60000 total_correct(sum): 57238 / 60000 loss: 467.4792817477137\n",
            "epoch 81 total_correct(images): 58673 / 60000 total_correct(sum): 57323 / 60000 loss: 462.0934607265517\n",
            "epoch 82 total_correct(images): 58680 / 60000 total_correct(sum): 57308 / 60000 loss: 452.2327322214842\n",
            "epoch 83 total_correct(images): 58724 / 60000 total_correct(sum): 57376 / 60000 loss: 442.929118885193\n",
            "epoch 84 total_correct(images): 58733 / 60000 total_correct(sum): 57465 / 60000 loss: 432.7389331795275\n",
            "epoch 85 total_correct(images): 58725 / 60000 total_correct(sum): 57505 / 60000 loss: 430.1324847515207\n",
            "epoch 86 total_correct(images): 58714 / 60000 total_correct(sum): 57626 / 60000 loss: 421.15098451427184\n",
            "epoch 87 total_correct(images): 58720 / 60000 total_correct(sum): 57602 / 60000 loss: 413.28731849836186\n",
            "epoch 88 total_correct(images): 58744 / 60000 total_correct(sum): 57632 / 60000 loss: 406.77272561425343\n",
            "epoch 89 total_correct(images): 58745 / 60000 total_correct(sum): 57650 / 60000 loss: 407.3495928780176\n",
            "epoch 90 total_correct(images): 58787 / 60000 total_correct(sum): 57699 / 60000 loss: 403.2898214738816\n",
            "epoch 91 total_correct(images): 58805 / 60000 total_correct(sum): 57705 / 60000 loss: 392.67584449541755\n",
            "epoch 92 total_correct(images): 58800 / 60000 total_correct(sum): 57769 / 60000 loss: 385.10193525324576\n",
            "epoch 93 total_correct(images): 58784 / 60000 total_correct(sum): 57814 / 60000 loss: 380.3785708574578\n",
            "epoch 94 total_correct(images): 58837 / 60000 total_correct(sum): 57850 / 60000 loss: 371.06351172341965\n",
            "epoch 95 total_correct(images): 58856 / 60000 total_correct(sum): 57866 / 60000 loss: 373.8245074665174\n",
            "epoch 96 total_correct(images): 58827 / 60000 total_correct(sum): 57892 / 60000 loss: 366.9756800418254\n",
            "epoch 97 total_correct(images): 58863 / 60000 total_correct(sum): 57904 / 60000 loss: 360.031832030043\n",
            "epoch 98 total_correct(images): 58855 / 60000 total_correct(sum): 57920 / 60000 loss: 357.20636436296627\n",
            "epoch 99 total_correct(images): 58859 / 60000 total_correct(sum): 57978 / 60000 loss: 353.28860045969486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJNjakS8aALD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a0b4cf-46c1-4bd5-e199-fd93c0fc7e1e"
      },
      "source": [
        "# validate the model #\n",
        "model.eval()\n",
        "for test_batch in test_loader: # Get Batch\n",
        "    images_test, randnos_test, labels_test, sums_test = test_batch\n",
        "    images_test, randnos_test, labels_test, sums_test = images_test.to(device), randnos_test.to(device), labels_test.to(device), sums_test.to(device)\n",
        "    test_preds1,test_preds2 = model(images_test,F.one_hot(torch.tensor(randnos_test), num_classes=10).float())\n",
        "    loss1_t = F.cross_entropy(test_preds1, labels_test) # Calculate Loss\n",
        "    loss2_t = F.cross_entropy(test_preds2, sums_test) # Calculate Loss\n",
        "    loss_t = (19*(loss1_t) + 10*(loss2_t))/29\n",
        "    total_loss_t += loss_t.item()\n",
        "    total_correct_images_t += get_num_correct(test_preds1, labels_test)\n",
        "    total_correct_sums_t += get_num_correct(test_preds2, sums_test)\n",
        "\n",
        "print(  \"total_correct(images):\", total_correct_images_t,\"/\",len(test_ds),\n",
        "        \"total_correct(sum):\" , total_correct_sums_t,\"/\",len(test_ds),\n",
        "        \"loss:\", total_loss_t\n",
        "    )\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_correct(images): 9763 / 10000 total_correct(sum): 9637 / 10000 loss: 72.99444600567222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83VXJVbJxAnC"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}